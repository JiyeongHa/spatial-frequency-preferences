{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import sfp\n",
    "import pyPyrTools as ppt\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy import optimize as opt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the motivation behind this model, let's step through some reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some setup for torch, loading in data\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "df_path = \"/scratch/wfb229/spatial_frequency_preferences/derivatives/first_level_analysis/stim_class/posterior/sub-wlsubj045/ses-02/sub-wlsubj045_ses-02_task-sfp_v1_e1-12_summary.csv\"\n",
    "df_path = \"/home/billbrod/Data/sub-wlsubj045_ses-02_task-sfp_v1_e1-12_summary.csv\"\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary for now, so we normalize values across voxels, but eventually we'll take care of this during the first level analysis\n",
    "gb = df.groupby(['varea', 'voxel'])\n",
    "df = df.set_index(['varea', 'voxel'])\n",
    "df['amplitude_estimate_norm'] = gb.amplitude_estimate_median.apply(np.linalg.norm, 2)\n",
    "df = df.reset_index()\n",
    "df['amplitude_estimate_median_normed'] = df.amplitude_estimate_median / df.amplitude_estimate_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some voxels that have good GLM $R^2$ values and pick one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.varea==1)&(df.R2>55)].drop_duplicates('voxel').sort_values('R2', ascending=False).head(20)[['voxel', 'R2', 'varea', 'hemi', 'angle' ,'eccen', 'precision']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a V1 voxel with a good R2\n",
    "voxel_df = df[(df.voxel.isin([264]))]#, 1421]))]\n",
    "voxel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.palettes.color_palette('deep', 5)\n",
    "pal = {'radial': pal[0], 'reverse spiral': pal[4], 'forward spiral': pal[2], 'angular': pal[3], 'mixtures': pal[1]}\n",
    "hue_order = ['radial', 'reverse spiral', 'forward spiral', 'angular', 'mixtures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the response of this voxel as a function of spatial frequency. In the plot below, we plot the normed amplitude estimate as a function of the local spatial frequency. We see that the response looks roughly log-Normal, but there appears to be some difference between the different stimulus classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(voxel_df[~voxel_df.stimulus_superclass.isin(['mixtures'])], hue='stimulus_superclass',palette=pal, size=8, aspect=1.5, hue_order=hue_order)\n",
    "g.map(plt.scatter, 'local_sf_magnitude', 'amplitude_estimate_median', linewidth=6)\n",
    "g.map(plt.plot, 'local_sf_magnitude', 'amplitude_estimate_median', linewidth=6)\n",
    "# g.ax.set_xscale('log', basex=2)\n",
    "g.add_legend()\n",
    "g.ax.tick_params(size=0)\n",
    "g.ax.set_xlim((0, 6))\n",
    "g.set_xlabels('Local spatial frequency (cpd)')\n",
    "g.set_ylabels('Amplitude Estimate')\n",
    "g.fig.savefig('voxel.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes differ in their local orientation, so we can look at a plot of the response as a function of the local spatial frequency with respect to x and y (size represents the response). This plot is difficult to parse, but the main point is that these different stimulus classes are not arbitrary and discrete: they lie on a continuum, related by the stimulus orientation, and so we can fit the response of the voxel as a 2d tuning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_sizes(x, y, s, plot_color=False, cmap=None, size_scale=1, **kwargs):\n",
    "    if plot_color:\n",
    "        kwargs.pop('color')\n",
    "        if cmap is None:\n",
    "            cmap = 'Blues'\n",
    "        plt.scatter(x, y, s=s*80*size_scale, c=s, cmap=cmap, **kwargs)\n",
    "    else:\n",
    "        plt.scatter(x, y, s=s*80*size_scale, **kwargs)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    voxel_df['normalized_resp'] = voxel_df['amplitude_estimate_median'].copy()\n",
    "    voxel_df['normalized_resp'] = (voxel_df['normalized_resp'] - voxel_df['normalized_resp'].min()) / (voxel_df['normalized_resp'].max() - voxel_df['normalized_resp'].min())\n",
    "    g=sns.FacetGrid(voxel_df, size=8, aspect=1, hue='stimulus_superclass', palette=pal, hue_order=hue_order)\n",
    "    g.map(scatter_sizes, 'local_w_x', 'local_w_y', 'normalized_resp', plot_color=False, size_scale=2)\n",
    "    g.add_legend()\n",
    "    scatter_ax = plt.gca()\n",
    "    scatter_ax.set_aspect('equal')\n",
    "    g.ax.tick_params(size=0)\n",
    "    g.fig.savefig('voxel-polar.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just that same data, but rotated and plotted on a semi-log plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid'), sns.plotting_context('notebook'):\n",
    "    g=sns.FacetGrid(voxel_df, hue='stimulus_superclass', size=5, aspect=1, palette=pal, hue_order=hue_order)\n",
    "    g.map(scatter_sizes, 'local_w_r', 'local_w_a', 'normalized_resp')\n",
    "    g.add_legend()\n",
    "    scatter_ax = plt.gca()\n",
    "    scatter_ax.set_xscale('symlog', basex=2, linthreshx=2**(-4))\n",
    "    scatter_ax.set_yscale('symlog', basey=2, linthreshy=2**(-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then the question is: how does the tuning change with orientation? Two possibilities are:\n",
    "\n",
    "1. The preferred frequency of the tuning curve / mode of the log-Gaussian distribution changes with orientation.\n",
    "2. The amplitude of the tuning curve changes with orientation.\n",
    "\n",
    "Then there's the question of how either the mode or the amplitude changes with orientation. Let's assume it changes smoothly and periodically, symmetrically about 180 degrees (because 2d orientation is runs from 0 to 180 degrees). We'll examine three possibilities in the plots below, from left to right:\n",
    "\n",
    "1. all orientation are equally important (mode/amplitude does not depend on orientation; constant)\n",
    "2. horizontal or vertical is preferred, but the other is anti-preferred (sinusoid with frequency $2\\theta$)\n",
    "3. the cardinals are preferred, the obliques are anti-preferred (sinusoid with frequency $4\\theta$)\n",
    "\n",
    "The following plots show these three possibilities, with the top showing a plot of orientation vs mode/response, while the bottom shows this on an x/y plot: the line represents either the mode or the level set of the max amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context('paper'), sns.axes_style('white'):\n",
    "    x = np.linspace(.1, 30, 100)\n",
    "    y = sfp.tuning_curves.log_norm_pdf(x,1, 6, .45)\n",
    "    plt.plot(x, y, 'k')\n",
    "    plt.savefig('loggauss2.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context('paper'):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    theta = np.linspace(0, np.pi, 1000)\n",
    "    for i, f, title in zip(range(3), [0, 2, 4], ['1. constant', '2. horizontal', '3. cardinals']):\n",
    "        r = (np.cos(f*theta) + 3) / 4\n",
    "        ax = plt.subplot(2,3,i+1)\n",
    "        ax.plot(theta, r)\n",
    "        ax.tick_params(size=0)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax = plt.subplot(2,3,4+i, projection='polar')\n",
    "        ax.plot(theta, r)\n",
    "    fig.savefig('variations.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so then how do we set up the function that we're going to fit? First, let's look at the 1d log-Normal distribution we used before. Normally, this distribution is parameterized by $\\mu$ and $\\sigma^2$. These are *not* the mean and variance, the way they are for the regular Normal distribution. We'll keep using $\\sigma^2$ but we'll use the mode, $M$ instead of $\\mu$: $M = \\exp(\\mu - \\sigma^2) \\Rightarrow \\mu = \\ln(M) + \\sigma^2$.\n",
    "\n",
    "Thus, when we're modeling the response, $R$ as a 1d log-Normal tuning curve with respect to the spatial frequency $\\omega$, it's: $R=A * \\exp(-\\frac{(\\ln(\\omega)-\\ln(M)-\\sigma^2)^2}{2\\sigma^2})$.\n",
    "\n",
    "Now we're extending this to make it 2d, as a funtion of spatial frequency $\\omega$ and orientation $\\theta$. To allow for the amplitude varying with orientation, we expand upon $A$ and make it orientation-dependent: $A_0 + A_1\\cos2\\theta + A_2\\cos4\\theta$. To allow the mode to vary, we make similarly make $M$ orientation-dependent: $M_0 + M_1\\cos2\\theta + M_2\\cos4\\theta$.\n",
    "\n",
    "Putting it altogether we get:\n",
    "\n",
    "$(A_0 + A_1\\cos2\\theta + A_2\\cos4\\theta)\\exp(-\\frac{(\\ln(\\omega)-\\ln(M_0 + M_1\\cos2\\theta + M_2\\cos4\\theta)-\\sigma^2)^2}{2\\sigma^2})$\n",
    "\n",
    "which gives us 7 parameters to fit: $A_0, A_1, A_2, M_0, M_1, M_2, \\sigma$\n",
    "\n",
    "Let's look at some examples of this function, to get a sense of its expressive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = np.logspace(-3, 3, 100, base=2)\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "def log_norm_2d(omega, theta, A0=1, A1=0, A2=0, M0=1, M1=0, M2=0, sigma=1):\n",
    "    omega = np.array(omega)\n",
    "    theta = np.array(theta)\n",
    "    amp = A0 + A1*np.cos(2*theta) + A2*np.cos(4*theta)\n",
    "    mode = M0 + M1*np.cos(2*theta) + M2*np.cos(4*theta)\n",
    "    mu = np.log(mode) + sigma**2\n",
    "    pdf = (1/(omega*sigma*np.sqrt(2*np.pi))) * np.exp(-(np.log(omega)-mu)**2/(2*sigma**2))\n",
    "    #pdf /= pdf.max()\n",
    "    return amp * pdf\n",
    "\n",
    "omega, theta = np.meshgrid(omega, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, subplot_kw={'projection': 'polar'}, figsize=(12, 6))\n",
    "\n",
    "params = [{}, {'M0': .5}, {'A1': .5}, {'M1': .5}, {'A1': .5, 'M1': .5, 'M0': 1}, {'A1': .5, 'M1': .5, 'M0':2}]\n",
    "\n",
    "with sns.plotting_context('notebook'):\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        R = log_norm_2d(omega, theta, **params[i])\n",
    "        ax.pcolormesh(theta, omega, R)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_rticks([])\n",
    "    fig.savefig('2d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is on a voxel-by-voxel basis, but what about across the whole area? To extend it to all of V1, let's consider two frames of reference: fixed and relative.\n",
    "\n",
    "In the fixed frame, all voxels have the same tuning. Orientation, $\\theta_f$, above refers to Cartesian, world-relate orientation so that $\\theta_f=0$ corresponds to \"to the right\". Spatial frequency, $\\omega_f$, means the local spatial frequency in the image. This encodes our \"constant\" extreme possibility from earlier.\n",
    "\n",
    "In the relative frame, voxel tuning depends on its location in the retinotopic map. We remap orientation and spatial frequency so that $\\theta_r=0$ corresponds to \"away from the fovea\" and spatial frequency is scaled by eccentricity: $\\omega_r=\\omega_f(e+b)$, where $e$ is the eccentricity of the voxel's pRF and $b$ is some constant.\n",
    "\n",
    "We then sum these two versions of the model so that we have a 15-parameter model (two versions of the 7 parameters above, plus $b$) that we fit simultaneously to all of V1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def torch_meshgrid(x, y=None):\n",
    "    \"\"\"from https://github.com/pytorch/pytorch/issues/7580\"\"\"\n",
    "    if y is None:\n",
    "        y = x\n",
    "    x = torch.tensor(x, dtype=torch.float64)\n",
    "    y = torch.tensor(y, dtype=torch.float64)\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    grid_x = x[None].expand(n, m)\n",
    "    grid_y = y[:, None].expand(n, m)\n",
    "    return grid_x, grid_y\n",
    "\n",
    "def _cast_as_param(x):\n",
    "    return torch.nn.Parameter(torch.tensor(x, dtype=torch.float64))\n",
    "\n",
    "class LogGaussianDonut(torch.nn.Module):\n",
    "    \"\"\"LogGaussianDonut in pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, major_axis_slope, minor_axis_slope, major_axis_sigma_slope, minor_axis_sigma_slope, rotation_angle_sf_coeff=0, rotation_angle_vox_coeff=0, major_axis_intercept=0, minor_axis_intercept=0, major_axis_sigma_intercept=0, minor_axis_sigma_intercept=0, rotation_angle_intercept=0):\n",
    "        super(LogGaussianDonut, self).__init__()\n",
    "        self.amplitude_dict = {}\n",
    "        self.major_axis_slope = _cast_as_param(major_axis_slope)\n",
    "        self.major_axis_intercept = _cast_as_param(major_axis_intercept)\n",
    "        self.minor_axis_slope = _cast_as_param(minor_axis_slope)\n",
    "        self.minor_axis_intercept = _cast_as_param(minor_axis_intercept)\n",
    "        self.major_axis_sigma_slope = _cast_as_param(major_axis_sigma_slope)\n",
    "        self.major_axis_sigma_intercept = _cast_as_param(major_axis_sigma_intercept)\n",
    "        self.minor_axis_sigma_slope = _cast_as_param(minor_axis_sigma_slope)\n",
    "        self.minor_axis_sigma_intercept = _cast_as_param(minor_axis_sigma_intercept)\n",
    "        self.rotation_angle_sf_coeff = _cast_as_param(rotation_angle_sf_coeff)\n",
    "        self.rotation_angle_vox_coeff = _cast_as_param(rotation_angle_vox_coeff)\n",
    "        self.rotation_angle_intercept = _cast_as_param(rotation_angle_intercept)\n",
    "    \n",
    "    def initialize_amplitude_dict(self, amp_dict):\n",
    "        amp_dict = list(amp_dict.iteritems())\n",
    "        self.amplitude_dict = dict(((\"%.05f\"%k[0], \"%.05f\"%k[1]), i) for i, (k, _) in enumerate(amp_dict))\n",
    "        self.amplitude_list = torch.nn.ParameterList([_cast_as_param([v]) for _, v in amp_dict])\n",
    "    \n",
    "    def _create_mag_angle(self, extent=(-10, 10), n_samps=1001):\n",
    "        x = torch.linspace(extent[0], extent[1], n_samps)\n",
    "        x, y = torch_meshgrid(x)\n",
    "        r = torch.sqrt(torch.pow(x, 2) + torch.pow(y, 2))\n",
    "        th = torch.atan2(y, x)\n",
    "        return r, th\n",
    "    \n",
    "    def create_image(self, vox_ecc, vox_angle, extent=None, n_samps=1001):\n",
    "        r, th = self._create_mag_angle(extent, n_samps)\n",
    "        return self.evaluate(r, th, vox_ecc, vox_angle)\n",
    "\n",
    "    def get_func_params(self, voxel_eccentricity, voxel_angle):\n",
    "        amplitude = []\n",
    "        try:\n",
    "            ecc = [i for i in voxel_eccentricity.cpu().numpy()]\n",
    "            ang = [i for i in voxel_angle.cpu().numpy()]\n",
    "        except TypeError:\n",
    "            ecc = [float(voxel_eccentricity.cpu().numpy())]\n",
    "            ang = [float(voxel_angle.cpu().numpy())]\n",
    "        for e, a in zip(ecc, ang):\n",
    "            amp_idx = self.amplitude_dict[(\"%.05f\"%e, \"%.05f\"%a)]\n",
    "            amplitude.append(self.amplitude_list[amp_idx])\n",
    "        amplitude = torch.cat(amplitude)\n",
    "        major_axis = 1. / (self.major_axis_slope * voxel_eccentricity + self.major_axis_intercept)\n",
    "        minor_axis = 1. / (self.minor_axis_slope * voxel_eccentricity + self.minor_axis_intercept)\n",
    "        major_axis_sigma = 1. / (self.major_axis_sigma_slope * voxel_eccentricity + self.major_axis_sigma_intercept)\n",
    "        minor_axis_sigma = 1. / (self.minor_axis_sigma_slope * voxel_eccentricity + self.minor_axis_sigma_intercept)\n",
    "        rotation_angle_func = lambda sf_angle: self.rotation_angle_sf_coeff * sf_angle + self.rotation_angle_vox_coeff * voxel_angle + self.rotation_angle_intercept\n",
    "        return amplitude, major_axis, minor_axis, major_axis_sigma, minor_axis_sigma, rotation_angle_func\n",
    "    \n",
    "    def evaluate(self, sf_mag, sf_angle, vox_ecc, vox_angle):\n",
    "        variables = {'sf_mag': sf_mag, 'sf_angle': sf_angle, 'vox_ecc': vox_ecc, 'vox_angle': vox_angle}\n",
    "        for k, v in variables.iteritems():\n",
    "            if not torch.is_tensor(v):\n",
    "                v = torch.tensor(v, dtype=torch.float64)\n",
    "            if self.major_axis_slope.is_cuda:\n",
    "                v = v.cuda()\n",
    "            variables[k] = v\n",
    "        amplitude, major_axis, minor_axis, major_axis_sigma, minor_axis_sigma, rotation_angle_func = self.get_func_params(variables['vox_ecc'], variables['vox_angle'])\n",
    "        rotation_angle = rotation_angle_func(variables['sf_angle'])\n",
    "        variables['sf_mag'] = torch.log2(variables['sf_mag'])\n",
    "        variables['sf_angle'] = variables['sf_angle'] + rotation_angle\n",
    "        # transform angles based on ellipse axes\n",
    "        variables['sf_angle'] = torch.atan2(major_axis*torch.sin(variables['sf_angle']), minor_axis*torch.cos(variables['sf_angle']))\n",
    "        # Gaussian center as function of angle\n",
    "        self.ctr = torch.sqrt(torch.pow(major_axis*torch.cos(variables['sf_angle']), 2) + torch.pow(minor_axis*torch.sin(variables['sf_angle']), 2))\n",
    "        # rotational sigma\n",
    "        self.sigma = torch.sqrt(torch.pow(major_axis_sigma*torch.cos(variables['sf_angle']), 2) + torch.pow(minor_axis_sigma*torch.sin(variables['sf_angle']),2))\n",
    "        # This is our function\n",
    "        return amplitude*torch.exp(-(variables['sf_mag']-torch.log2(self.ctr))**2 / (2*self.sigma**2))\n",
    "\n",
    "    def forward(self, spatial_frequency_magnitude, spatial_frequency_theta, voxel_eccentricity, voxel_angle):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.evaluate(spatial_frequency_magnitude, spatial_frequency_theta, voxel_eccentricity, voxel_angle)\n",
    "    \n",
    "    \n",
    "class ConstantLogGaussianDonut(LogGaussianDonut):\n",
    "    \"\"\"this version does not depend on voxel eccentricity or angle at all, mainly for demo purposes\"\"\"\n",
    "    def __init__(self, amplitude, major_axis, minor_axis, major_axis_sigma, minor_axis_sigma, rotation_angle):\n",
    "        # This way, the {major|minor}_axis(_sigma) is what the user specifies (since we takes the inverse above)\n",
    "        super(ConstantLogGaussianDonut, self).__init__(0, 0, 0, 0, 0, 0, 1./major_axis, 1./minor_axis, 1./major_axis_sigma, 1./minor_axis_sigma, rotation_angle)\n",
    "        self.initialize_amplitude_dict({(0, 0): amplitude})\n",
    "        \n",
    "    def create_image(self, extent=None, n_samps=1001):\n",
    "        r, th = self._create_mag_angle(extent, n_samps)\n",
    "        return self.evaluate(r, th)\n",
    "        \n",
    "    def evaluate(self, sf_mag, sf_angle):\n",
    "        return super(ConstantLogGaussianDonut, self).evaluate(sf_mag, sf_angle, 0, 0)\n",
    "        \n",
    "    def forward(self, spatial_frequency_magnitude, spatial_frequency_theta):\n",
    "        return self.evaluate(spatial_frequency_magnitude, spatial_frequency_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut = ConstantLogGaussianDonut(20, 1, 1, .5, .5,0).to(device)\n",
    "x = np.linspace(-5, 5, 1001)\n",
    "xgrid, ygrid = np.meshgrid(x, x)\n",
    "# detach() is required to separate it from the graph implied by setting `requires_grad=True` above\n",
    "plt.imshow(donut.create_image((x.min(), x.max())).detach(), extent=(x.min(),x.max(), x.min(), x.max()), cmap='Reds')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut = LogGaussianDonut(1, 1, 1./.5, 1/.5, 0)\n",
    "donut.initialize_amplitude_dict({(1, 0): 20})\n",
    "donut.to(device)\n",
    "x = np.linspace(-5, 5, 1001)\n",
    "xgrid, ygrid = np.meshgrid(x, x)\n",
    "# detach() is required to separate it from the graph implied by setting `requires_grad=True` above\n",
    "plt.imshow(donut.create_image(1, 0, (x.min(), x.max())).detach(), extent=(x.min(),x.max(), x.min(), x.max()), cmap='Reds')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = donut.create_image(1, 0, (x.min(), x.max()), len(x)).cpu().detach().numpy()\n",
    "fig, axes = plt.subplots(2,2,figsize=(10, 5))\n",
    "axes=axes.flatten()\n",
    "R = ppt.mkR(len(x))\n",
    "R *= (np.sqrt(2*x.max()**2)/R.max())\n",
    "R[xgrid<0] *= -1\n",
    "for ax, a in zip(axes.flatten(), [0, 1, 2, 3]):\n",
    "    idx = np.where(xgrid==a*ygrid)\n",
    "    r = R[idx]\n",
    "    ax.plot(r, img[idx])\n",
    "    ax.set(xlim=(-8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_amplitude_init_dict(df):\n",
    "    df = df.groupby('voxel')[['amplitude_estimate_median', 'eccen', 'angle']].max()\n",
    "    df = df.reset_index().set_index(['eccen', 'angle'])\n",
    "    amp_dict = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        amp_dict[idx] = row.amplitude_estimate_median\n",
    "    return amp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = FirstLevelDataset(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.FacetGrid(voxel_df, size=5, aspect=1)\n",
    "g.map(scatter_sizes, 'local_w_x', 'local_w_y', 'normalized_resp')\n",
    "scatter_ax = plt.gca()\n",
    "scatter_ax.set_aspect('equal')\n",
    "\n",
    "mag = torch.tensor(voxel_df.local_sf_magnitude.values, dtype=torch.float64).to(device)\n",
    "direc = torch.tensor(voxel_df.local_sf_xy_direction.values, dtype=torch.float64).to(device)\n",
    "ecc = torch.tensor(voxel_df.eccen.values, dtype=torch.float64).to(device)\n",
    "ang = torch.tensor(voxel_df.angle.values, dtype=torch.float64).to(device)\n",
    "y = torch.tensor(voxel_df.amplitude_estimate_median.values, dtype=torch.float64).to(device)\n",
    "\n",
    "donut = LogGaussianDonut(1, 1, .2, .2)\n",
    "donut.initialize_amplitude_dict(create_amplitude_init_dict(ds.df))\n",
    "donut.to(device)\n",
    "x = np.linspace(-3, 3, 101)\n",
    "# detach() is required to separate it from the graph implied by setting `requires_grad=True` above\n",
    "c = scatter_ax.contour(x, x, donut.create_image(ecc[0], ang[0], (x.min(), x.max()), len(x)).detach(), cmap=\"Reds\")\n",
    "g.fig.colorbar(c, shrink=.5)\n",
    "scatter_ax.set(xlim=(-4.5, 4.5), ylim=(-.5,5))\n",
    "scatter_ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(False)\n",
    "optimizer = torch.optim.Adam(donut.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(donut.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data as torchdata\n",
    "class FirstLevelDataset(torchdata.Dataset):\n",
    "    def __init__(self, df_path, direction_type='relative'):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        if direction_type not in ['relative', 'absolute']:\n",
    "            raise Exception(\"Don't know how to handle direction_type %s!\" % direction_type)\n",
    "        self.direction_type = direction_type\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = df.iloc[idx]\n",
    "        if self.direction_type == 'relative':\n",
    "            vals = row[['local_sf_magnitude', 'local_sf_ra_direction', 'eccen', 'angle']].values\n",
    "        elif self.direction_type == 'absolute':\n",
    "            vals = row[['local_sf_magnitude', 'local_sf_xy_direction', 'eccen', 'angle']].values\n",
    "        feature = torch.tensor(vals.astype(float), dtype=torch.float64)\n",
    "        try:\n",
    "            target = torch.tensor(row['amplitude_estimate'], dtype=torch.float64)\n",
    "        except KeyError:\n",
    "            target = torch.tensor(row['amplitude_estimate_median'], dtype=torch.float64)\n",
    "        return feature, target\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torchdata.DataLoader(ds, 50)\n",
    "loss_prev = 0.01\n",
    "n_epochs = 5\n",
    "thresh = .00001\n",
    "for t in range(n_epochs):\n",
    "    for i, (features, target) in enumerate(dl):\n",
    "        predictions = donut(*features.transpose(1, 0))\n",
    "        loss = loss_fn(predictions, target.to(device))\n",
    "        if i % 100 == 0:\n",
    "            print(i, loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if abs((loss - loss_prev) / loss_prev) < thresh:\n",
    "            break\n",
    "        loss_prev = loss\n",
    "print(\"Final loss: %02f\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(donut.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20,5))\n",
    "vals = [y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), y.cpu().detach().numpy() - y_pred.cpu().detach().numpy()]\n",
    "titles = ['ground truth', 'predicted', 'ground truth - predicted']\n",
    "for ax, v, t in zip(axes.flatten(), vals, titles):\n",
    "    scaled_v = (v - abs(v).min()) / (abs(v).max() - abs(v).min())\n",
    "    pts=ax.scatter(voxel_df['local_w_x'], voxel_df['local_w_y'], s=abs(scaled_v)*50, c=v, cmap='RdBu_r', norm=sfp.plotting.MidpointNormalize(midpoint=0))\n",
    "    ax.set_aspect('equal')\n",
    "    plt.colorbar(pts, ax=ax, shrink=.6)\n",
    "    ax.set(xlim=(-4.5, 4.5), ylim=(-.5, 5))\n",
    "    ax.set_title(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8, 8, 1001)\n",
    "# detach() is required to separate it from the graph implied by setting `requires_grad=True` above\n",
    "plt.imshow(donut.create_image((x.min(), x.max())).cpu().detach(), extent=(x.min(),x.max(), x.min(), x.max()),cmap='RdBu_r', norm=sfp.plotting.MidpointNormalize(midpoint=0))\n",
    "ax = plt.gca()\n",
    "ax.set(xlim=(-4.5, 4.5), ylim=(-.5, 5))\n",
    "plt.colorbar(shrink=.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
